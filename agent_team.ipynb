{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddeec8ff",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e96684b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install google-adk -q\n",
    "%pip install litellm -q\n",
    "\n",
    "print(\"Installation complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009e615d",
   "metadata": {},
   "source": [
    "### Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84dddce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import asyncio\n",
    "from google.adk.agents import Agent\n",
    "from google.adk.models.lite_llm import LiteLlm # For multi-model support\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "from google.adk.runners import Runner\n",
    "from google.genai import types # For creating message Content/Parts\n",
    "\n",
    "import warnings\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "print(\"Libraries imported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c0d956",
   "metadata": {},
   "source": [
    "### Configuring API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92fbe0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Keys Set:\n",
      "Google API Key set: Yes\n",
      "OpenAI API Key set: No (REPLACE PLACEHOLDER!)\n",
      "Anthropic API Key set: No (REPLACE PLACEHOLDER!)\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# --- IMPORTANT: Replace placeholders with your real API keys ---\n",
    "\n",
    "# Gemini API Key (Get from Google AI Studio: https://aistudio.google.com/app/apikey)\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\") # <--- REPLACE\n",
    "\n",
    "# OpenAI API Key (Get from OpenAI Platform: https://platform.openai.com/api-keys)\n",
    "os.environ['OPENAI_API_KEY'] = 'YOUR_OPENAI_API_KEY' # <--- REPLACE\n",
    "\n",
    "# Anthropic API Key (Get from Anthropic Console: https://console.anthropic.com/settings/keys)\n",
    "os.environ['ANTHROPIC_API_KEY'] = 'YOUR_ANTHROPIC_API_KEY' # <--- REPLACE\n",
    "\n",
    "\n",
    "# --- Verify Keys (Optional Check) ---\n",
    "print(\"API Keys Set:\")\n",
    "print(f\"Google API Key set: {'Yes' if os.environ.get('GOOGLE_API_KEY') and os.environ['GOOGLE_API_KEY'] != 'YOUR_GOOGLE_API_KEY' else 'No (REPLACE PLACEHOLDER!)'}\")\n",
    "print(f\"OpenAI API Key set: {'Yes' if os.environ.get('OPENAI_API_KEY') and os.environ['OPENAI_API_KEY'] != 'YOUR_OPENAI_API_KEY' else 'No (REPLACE PLACEHOLDER!)'}\")\n",
    "print(f\"Anthropic API Key set: {'Yes' if os.environ.get('ANTHROPIC_API_KEY') and os.environ['ANTHROPIC_API_KEY'] != 'YOUR_ANTHROPIC_API_KEY' else 'No (REPLACE PLACEHOLDER!)'}\")\n",
    "\n",
    "# Configure ADK to use API keys directly (not Vertex AI for this multi-model setup)\n",
    "os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = \"False\"\n",
    "\n",
    "# @markdown **Security Note:** It's best practice to manage API keys securely (e.g., using Colab Secrets or environment variables) rather than hardcoding them directly in the notebook. Replace the placeholder strings above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914891c2",
   "metadata": {},
   "source": [
    "### Defining Model Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "636e4351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Environment configured.\n"
     ]
    }
   ],
   "source": [
    "MODEL_GEMINI_2_0_FLASH = \"gemini-2.0-flash\"\n",
    "\n",
    "# This seemed to work better\n",
    "MODEL_GEMINI_2_0_FLASH_EXP = \"gemini-2.0-flash-exp\"\n",
    "\n",
    "# Note: Specific model names might change. Refer to LiteLLM or the model provider's documentation.\n",
    "MODEL_GPT_4O = \"openai/gpt-4o\"\n",
    "MODEL_CLAUDE_SONNET = \"anthropic/claude-3-sonnet-20240229\"\n",
    "\n",
    "\n",
    "print(\"\\nEnvironment configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5904637",
   "metadata": {},
   "source": [
    "# Building First Agent\n",
    "\n",
    "**Core Principles**\n",
    "\n",
    "- Agent: The underlying \"brain\" that communicates with the user and determines what to do depending on user requests\n",
    "- Tool: Python functions that gives the agent to perform specific tasks. We can have functions to check time, look up weather, send emails, etc. Tools should be detailed and very systematic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bbbf43",
   "metadata": {},
   "source": [
    "### Defining Tools\n",
    "\n",
    "**Key Concepts**: Docstrings are crucial, as good ones allow agents to better under **how** to use them and to understand:\n",
    "- what the tool does\n",
    "- when to use it\n",
    "- what arguments it requires\n",
    "- what information it returns\n",
    "\n",
    "**Best Practice**: Write clear, descriptive, and accurate docstring for tools. (There's a potential for ChatGPT to help write the docstrings, since they're capable of being descriptive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0f75b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Tool: get_weather called for city: New York ---\n",
      "{'status': 'success', 'report': 'The weather in New York is sunny with a temperature of 25째C.'}\n",
      "--- Tool: get_weather called for city: Paris ---\n",
      "{'status': 'error', 'error_message': \"Sorry, I don't have weather information for 'Paris'.\"}\n"
     ]
    }
   ],
   "source": [
    "# @title Define the get_weather Tool\n",
    "def get_weather(city: str) -> dict:\n",
    "    \"\"\"Retrieves the current weather report for a specified city.\n",
    "\n",
    "    Args:\n",
    "        city (str): The name of the city (e.g., \"New York\", \"London\", \"Tokyo\").\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the weather information.\n",
    "              Includes a 'status' key ('success' or 'error').\n",
    "              If 'success', includes a 'report' key with weather details.\n",
    "              If 'error', includes an 'error_message' key.\n",
    "    \"\"\"\n",
    "    # Best Practice: Log tool execution for easier debugging\n",
    "    print(f\"--- Tool: get_weather called for city: {city} ---\")\n",
    "    city_normalized = city.lower().replace(\" \", \"\") # Basic input normalization\n",
    "\n",
    "    # Mock weather data for simplicity\n",
    "    mock_weather_db = {\n",
    "        \"newyork\": {\"status\": \"success\", \"report\": \"The weather in New York is sunny with a temperature of 25째C.\"},\n",
    "        \"london\": {\"status\": \"success\", \"report\": \"It's cloudy in London with a temperature of 15째C.\"},\n",
    "        \"tokyo\": {\"status\": \"success\", \"report\": \"Tokyo is experiencing light rain and a temperature of 18째C.\"},\n",
    "    }\n",
    "\n",
    "    # Best Practice: Handle potential errors gracefully within the tool\n",
    "    if city_normalized in mock_weather_db:\n",
    "        return mock_weather_db[city_normalized]\n",
    "    else:\n",
    "        return {\"status\": \"error\", \"error_message\": f\"Sorry, I don't have weather information for '{city}'.\"}\n",
    "\n",
    "# Example tool usage (optional self-test)\n",
    "print(get_weather(\"New York\"))\n",
    "print(get_weather(\"Paris\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd6cd32",
   "metadata": {},
   "source": [
    "# Defining Agents\n",
    "\n",
    "An orchestrator that facilitates interaction between the user and LLM and available tools\n",
    "\n",
    "For ADK, there's several key parameters for an agent:\n",
    "- `name`: A unique identifier for this agent\n",
    "- `model`: Specifies what model this agent should use. Different models may be more capable than others had doing certain tasks\n",
    "- `description`: A concise sumnmary of the agent's overall purpose, which is crucial later when other agents need to decide whether to delegate tasks to this agent.\n",
    "- `instruction`: Detailed guidance for the LLM on how to behave, its persona, its goals, and specifically how and when to utilize its assigned `tools`\n",
    "- `tools`: A list containing the actual Python tool functions that the agent is allowed to use\n",
    "\n",
    "**Best Practice**: Choose descriptive `name` and `description` values, since those are used internally by ADK and are vital for features like automic delegation\n",
    "\n",
    "**Note**: In a way, the agents are structured like a tree hierachy, where task are first passed to the root agent, then it gets sent to respective agents that are most suited for the task. An analogy can be like a company and how tasks are dealt with. The CEO sees a tasks, then it decides to pass it to a particular department, then the department leader pass to a subgroup to handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0094fd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 'weather_agent_v1' created using model 'gemini-2.0-flash-exp'.\n"
     ]
    }
   ],
   "source": [
    "# @title Define the Weather Agent\n",
    "# Use one of the model constants defined earlier\n",
    "AGENT_MODEL = MODEL_GEMINI_2_0_FLASH_EXP \n",
    "\n",
    "weather_agent = Agent(\n",
    "    name=\"weather_agent_v1\",\n",
    "    model=AGENT_MODEL, # Specifies the underlying LLM\n",
    "    description=\"Provides weather information for specific cities.\", # Crucial for delegation later\n",
    "    instruction=\"You are a helpful weather assistant. Your primary goal is to provide current weather reports. \"\n",
    "                \"When the user asks for the weather in a specific city, \"\n",
    "                \"you MUST use the 'get_weather' tool to find the information. \"\n",
    "                \"Analyze the tool's response: if the status is 'error', inform the user politely about the error message. \"\n",
    "                \"If the status is 'success', present the weather 'report' clearly and concisely to the user. \"\n",
    "                \"Only use the tool when a city is mentioned for a weather request.\",\n",
    "    tools=[get_weather], # Make the tool available to this agent\n",
    ")\n",
    "\n",
    "print(f\"Agent '{weather_agent.name}' created using model '{AGENT_MODEL}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f394e0",
   "metadata": {},
   "source": [
    "### Setup Runner and Session Servicess\n",
    "\n",
    "This is to manage conversation and execute the agent, and has 2 components:\n",
    "\n",
    "- `SessionService`: Responsible for managing conversation history and state for different user and sessions. In the following, we'll use `InMemorySessionService`, a simple implementation that stores everything in memory, suitable for testing and simple applications. It also keeps track of messages exchanged\n",
    "- `Runner`: The engine that orchestrates the interaction flow. It takes user input, route it to the appropriate agent, manages calls to the LLM and tools based on the agent's logic, handles session updates via the `SessionService`, and yields events representing the process of the interaction\n",
    "\n",
    "In short, the `SessionService` manages conversation history for different users, and the `Runner` is the facilitator that direct the flow of user inputs, determining which sub-agents are the most appropriate for a particular task\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb979095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session created: App='weather_tutorial_app', User='user_1', Session='session_001'\n",
      "Runner created for agent 'weather_agent_v1'.\n"
     ]
    }
   ],
   "source": [
    "# @title Setup Session Service and Runner\n",
    "\n",
    "# --- Session Management ---\n",
    "# Key Concept: SessionService stores conversation history & state.\n",
    "# InMemorySessionService is simple, non-persistent storage for this tutorial.\n",
    "session_service = InMemorySessionService()\n",
    "\n",
    "# Define constants for identifying the interaction context\n",
    "APP_NAME = \"weather_tutorial_app\"\n",
    "USER_ID = \"user_1\"\n",
    "SESSION_ID = \"session_001\" # Using a fixed ID for simplicity\n",
    "\n",
    "# Create the specific session where the conversation will happen\n",
    "session = session_service.create_session(\n",
    "    app_name=APP_NAME,\n",
    "    user_id=USER_ID,\n",
    "    session_id=SESSION_ID\n",
    ")\n",
    "print(f\"Session created: App='{APP_NAME}', User='{USER_ID}', Session='{SESSION_ID}'\")\n",
    "\n",
    "# --- Runner ---\n",
    "# Key Concept: Runner orchestrates the agent execution loop.\n",
    "runner = Runner(\n",
    "    agent=weather_agent, # The agent we want to run\n",
    "    app_name=APP_NAME,   # Associates runs with our app\n",
    "    session_service=session_service # Uses our session manager\n",
    ")\n",
    "print(f\"Runner created for agent '{runner.agent.name}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826cc2e1",
   "metadata": {},
   "source": [
    "Note: When looking at the above code, it seems like Runner is given an agent and a session. Thus the runner is specific to that particular session, as well as having access to that specific agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5f8accf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Define Agent Interaction Function\n",
    "import asyncio\n",
    "from google.genai import types # For creating message Content/Parts\n",
    "\n",
    "async def call_agent_async(query: str):\n",
    "  \"\"\"Sends a query to the agent and prints the final response.\"\"\"\n",
    "  print(f\"\\n>>> User Query: {query}\")\n",
    "\n",
    "  # Prepare the user's message in ADK format\n",
    "  content = types.Content(role='user', parts=[types.Part(text=query)])\n",
    "\n",
    "  final_response_text = \"Agent did not produce a final response.\" # Default\n",
    "\n",
    "  # Key Concept: run_async executes the agent logic and yields Events.\n",
    "  # We iterate through events to find the final answer.\n",
    "  async for event in runner.run_async(user_id=USER_ID, session_id=SESSION_ID, new_message=content):\n",
    "      # You can uncomment the line below to see *all* events during execution\n",
    "      # print(f\"  [Event] Author: {event.author}, Type: {type(event).__name__}, Final: {event.is_final_response()}, Content: {event.content}\")\n",
    "\n",
    "      # Key Concept: is_final_response() marks the concluding message for the turn.\n",
    "      if event.is_final_response():\n",
    "          if event.content and event.content.parts:\n",
    "             # Assuming text response in the first part\n",
    "             final_response_text = event.content.parts[0].text\n",
    "          elif event.actions and event.actions.escalate: # Handle potential errors/escalations\n",
    "             final_response_text = f\"Agent escalated: {event.error_message or 'No specific message.'}\"\n",
    "          # Add more checks here if needed (e.g., specific error codes)\n",
    "          break # Stop processing events once the final response is found\n",
    "\n",
    "  print(f\"<<< Agent Response: {final_response_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66e474c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> User Query: What is the weather like in London?\n",
      "--- Tool: get_weather called for city: London ---\n",
      "<<< Agent Response: It's cloudy in London with a temperature of 15째C.\n",
      "\n",
      "\n",
      ">>> User Query: How about Paris?\n",
      "--- Tool: get_weather called for city: Paris ---\n",
      "<<< Agent Response: Sorry, I don't have weather information for 'Paris'.\n",
      "\n",
      "\n",
      ">>> User Query: Tell me the weather in New York\n",
      "--- Tool: get_weather called for city: New York ---\n",
      "<<< Agent Response: The weather in New York is sunny with a temperature of 25째C.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title Run the Initial Conversation\n",
    "\n",
    "# We need an async function to await our interaction helper\n",
    "async def run_conversation():\n",
    "    await call_agent_async(\"What is the weather like in London?\")\n",
    "    await call_agent_async(\"How about Paris?\") # Expecting the tool's error message\n",
    "    await call_agent_async(\"Tell me the weather in New York\")\n",
    "\n",
    "# Execute the conversation using await in an async context (like Colab/Jupyter)\n",
    "await run_conversation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2eea3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
