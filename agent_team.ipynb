{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddeec8ff",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e96684b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installation complete.\n"
     ]
    }
   ],
   "source": [
    "# %pip install google-adk -q\n",
    "# %pip install litellm -q\n",
    "\n",
    "print(\"Installation complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009e615d",
   "metadata": {},
   "source": [
    "### Importing Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84dddce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported.\n"
     ]
    }
   ],
   "source": [
    "# @title Import necessary libraries\n",
    "import os\n",
    "import asyncio\n",
    "from google.adk.agents import Agent\n",
    "from google.adk.models.lite_llm import LiteLlm # For multi-model support\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "from google.adk.runners import Runner\n",
    "from google.genai import types # For creating message Content/Parts\n",
    "\n",
    "import warnings\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "print(\"Libraries imported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c0d956",
   "metadata": {},
   "source": [
    "### Configuring API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "92fbe0da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Keys Set:\n",
      "Google API Key set: Yes\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "\n",
    "# --- Verify Keys ---\n",
    "print(\"API Keys Set:\")\n",
    "print(f\"Google API Key set: {'Yes' if os.environ.get('GOOGLE_API_KEY') and os.environ['GOOGLE_API_KEY'] != 'YOUR_GOOGLE_API_KEY' else 'No (REPLACE PLACEHOLDER!)'}\")\n",
    "\n",
    "# Configure ADK to use API keys directly (not Vertex AI for this multi-model setup)\n",
    "os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = \"False\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914891c2",
   "metadata": {},
   "source": [
    "### Defining Model Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "636e4351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Environment configured.\n"
     ]
    }
   ],
   "source": [
    "MODEL_GEMINI_2_0_FLASH = \"gemini-2.0-flash\"\n",
    "\n",
    "# This seemed to work better\n",
    "MODEL_GEMINI_2_0_FLASH_EXP = \"gemini-2.0-flash-exp\"\n",
    "\n",
    "\n",
    "print(\"\\nEnvironment configured.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5904637",
   "metadata": {},
   "source": [
    "# Building First Agent\n",
    "\n",
    "**Core Principles**\n",
    "\n",
    "- Agent: The underlying \"brain\" that communicates with the user and determines what to do depending on user requests\n",
    "- Tool: Python functions that gives the agent to perform specific tasks. We can have functions to check time, look up weather, send emails, etc. Tools should be detailed and very systematic\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bbbf43",
   "metadata": {},
   "source": [
    "### Defining Tools\n",
    "\n",
    "**Key Concepts**: Docstrings are crucial, as good ones allow agents to better under **how** to use them and to understand:\n",
    "- what the tool does\n",
    "- when to use it\n",
    "- what arguments it requires\n",
    "- what information it returns\n",
    "\n",
    "**Best Practice**: Write clear, descriptive, and accurate docstring for tools. (There's a potential for ChatGPT to help write the docstrings, since they're capable of being descriptive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f0f75b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Tool: get_weather called for city: New York ---\n",
      "{'status': 'success', 'report': 'The weather in New York is sunny with a temperature of 25째C.'}\n",
      "--- Tool: get_weather called for city: Paris ---\n",
      "{'status': 'error', 'error_message': \"Sorry, I don't have weather information for 'Paris'.\"}\n"
     ]
    }
   ],
   "source": [
    "# @title Define the get_weather Tool\n",
    "def get_weather(city: str) -> dict:\n",
    "    \"\"\"Retrieves the current weather report for a specified city.\n",
    "\n",
    "    Args:\n",
    "        city (str): The name of the city (e.g., \"New York\", \"London\", \"Tokyo\").\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the weather information.\n",
    "              Includes a 'status' key ('success' or 'error').\n",
    "              If 'success', includes a 'report' key with weather details.\n",
    "              If 'error', includes an 'error_message' key.\n",
    "    \"\"\"\n",
    "    print(f\"--- Tool: get_weather called for city: {city} ---\") # Log tool execution\n",
    "    city_normalized = city.lower().replace(\" \", \"\") # Basic normalization\n",
    "\n",
    "    # Mock weather data\n",
    "    mock_weather_db = {\n",
    "        \"newyork\": {\"status\": \"success\", \"report\": \"The weather in New York is sunny with a temperature of 25째C.\"},\n",
    "        \"london\": {\"status\": \"success\", \"report\": \"It's cloudy in London with a temperature of 15째C.\"},\n",
    "        \"tokyo\": {\"status\": \"success\", \"report\": \"Tokyo is experiencing light rain and a temperature of 18째C.\"},\n",
    "    }\n",
    "\n",
    "    if city_normalized in mock_weather_db:\n",
    "        return mock_weather_db[city_normalized]\n",
    "    else:\n",
    "        return {\"status\": \"error\", \"error_message\": f\"Sorry, I don't have weather information for '{city}'.\"}\n",
    "\n",
    "# Example tool usage (optional test)\n",
    "print(get_weather(\"New York\"))\n",
    "print(get_weather(\"Paris\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd6cd32",
   "metadata": {},
   "source": [
    "# Defining Agents\n",
    "\n",
    "An orchestrator that facilitates interaction between the user and LLM and available tools\n",
    "\n",
    "For ADK, there's several key parameters for an agent:\n",
    "- `name`: A unique identifier for this agent\n",
    "- `model`: Specifies what model this agent should use. Different models may be more capable than others had doing certain tasks\n",
    "- `description`: A concise sumnmary of the agent's overall purpose, which is crucial later when other agents need to decide whether to delegate tasks to this agent.\n",
    "- `instruction`: Detailed guidance for the LLM on how to behave, its persona, its goals, and specifically how and when to utilize its assigned `tools`\n",
    "- `tools`: A list containing the actual Python tool functions that the agent is allowed to use\n",
    "\n",
    "**Best Practice**: Choose descriptive `name` and `description` values, since those are used internally by ADK and are vital for features like automic delegation\n",
    "\n",
    "**Note**: In a way, the agents are structured like a tree hierachy, where task are first passed to the root agent, then it gets sent to respective agents that are most suited for the task. An analogy can be like a company and how tasks are dealt with. The CEO sees a tasks, then it decides to pass it to a particular department, then the department leader pass to a subgroup to handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0094fd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 'weather_agent_v1' created using model 'gemini-2.0-flash'.\n"
     ]
    }
   ],
   "source": [
    "# @title Define the Weather Agent\n",
    "# Use one of the model constants defined earlier\n",
    "AGENT_MODEL = MODEL_GEMINI_2_0_FLASH\n",
    "\n",
    "weather_agent = Agent(\n",
    "    name=\"weather_agent_v1\",\n",
    "    model=AGENT_MODEL, # Can be a string for Gemini or a LiteLlm object\n",
    "    description=\"Provides weather information for specific cities.\",\n",
    "    instruction=\"You are a helpful weather assistant. \"\n",
    "                \"When the user asks for the weather in a specific city, \"\n",
    "                \"use the 'get_weather' tool to find the information. \"\n",
    "                \"If the tool returns an error, inform the user politely. \"\n",
    "                \"If the tool is successful, present the weather report clearly.\",\n",
    "    tools=[get_weather], # Pass the function directly\n",
    ")\n",
    "\n",
    "print(f\"Agent '{weather_agent.name}' created using model '{AGENT_MODEL}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f394e0",
   "metadata": {},
   "source": [
    "### Setup Runner and Session Servicess\n",
    "\n",
    "This is to manage conversation and execute the agent, and has 2 components:\n",
    "\n",
    "- `SessionService`: Responsible for managing conversation history and state for different user and sessions. In the following, we'll use `InMemorySessionService`, a simple implementation that stores everything in memory, suitable for testing and simple applications. It also keeps track of messages exchanged\n",
    "- `Runner`: The engine that orchestrates the interaction flow. It takes user input, route it to the appropriate agent, manages calls to the LLM and tools based on the agent's logic, handles session updates via the `SessionService`, and yields events representing the process of the interaction\n",
    "\n",
    "In short, the `SessionService` manages conversation history for different users, and the `Runner` is the facilitator that direct the flow of user inputs, determining which sub-agents are the most appropriate for a particular task\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb979095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session created: App='weather_tutorial_app', User='user_1', Session='session_001'\n",
      "Runner created for agent 'weather_agent_v1'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "session_service = InMemorySessionService()\n",
    "\n",
    "# Define constants for identifying the interaction context\n",
    "APP_NAME = \"weather_tutorial_app\"\n",
    "USER_ID = \"user_1\"\n",
    "SESSION_ID = \"session_001\" # Using a fixed ID for simplicity\n",
    "\n",
    "# Create the specific session where the conversation will happen\n",
    "session = session_service.create_session(\n",
    "    app_name=APP_NAME,\n",
    "    user_id=USER_ID,\n",
    "    session_id=SESSION_ID\n",
    ")\n",
    "print(f\"Session created: App='{APP_NAME}', User='{USER_ID}', Session='{SESSION_ID}'\")\n",
    "\n",
    "# --- Runner ---\n",
    "# Key Concept: Runner orchestrates the agent execution loop.\n",
    "runner = Runner(\n",
    "    agent=weather_agent, # The agent we want to run\n",
    "    app_name=APP_NAME,   # Associates runs with our app\n",
    "    session_service=session_service # Uses our session manager\n",
    ")\n",
    "print(f\"Runner created for agent '{runner.agent.name}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826cc2e1",
   "metadata": {},
   "source": [
    "Note: When looking at the above code, it seems like Runner is given an agent and a session. Thus the runner is specific to that particular session, as well as having access to that specific agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93be6b7d",
   "metadata": {},
   "source": [
    "### Interacting with the Agent\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa22e137",
   "metadata": {},
   "source": [
    "We need a way to send messages to our agents and recieve its responses. Since LLM calls and tool execution can take time, ADK's `Runner` operates asynchronously. \n",
    "\n",
    "A helper function `call_agent_async` will be used to:\n",
    "- Takes a a user query string\n",
    "- Packages it into ADK `Content` format (Its expected input)\n",
    "- Calls `runner.run.async`, providing the user/session context and the new message\n",
    "- Iterates through the Event yielded by the runner. Events represent steps in the agent's execution (e.g. tool call requested, tool result recieved, intermediate LLM thought, final response)\n",
    "- Identifies and prints the final response event using `event.is_final_response()`\n",
    "\n",
    "**Purpose of `async`**: Interactions with LLMs and some tools are I/O-bound operations. Using `asyncio` allows the program to handle these operations efficiently without blocking execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e5f8accf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Define Agent Interaction Function\n",
    "\n",
    "from google.genai import types # For creating message Content/Parts\n",
    "\n",
    "async def call_agent_async(query: str, runner, user_id, session_id):\n",
    "  \"\"\"Sends a query to the agent and prints the final response.\"\"\"\n",
    "  print(f\"\\n>>> User Query: {query}\")\n",
    "\n",
    "  # Prepare the user's message in ADK format\n",
    "  content = types.Content(role='user', parts=[types.Part(text=query)])\n",
    "\n",
    "  final_response_text = \"Agent did not produce a final response.\" # Default\n",
    "\n",
    "  # Key Concept: run_async executes the agent logic and yields Events.\n",
    "  # We iterate through events to find the final answer.\n",
    "  async for event in runner.run_async(user_id=user_id, session_id=session_id, new_message=content):\n",
    "      # You can uncomment the line below to see *all* events during execution\n",
    "      # print(f\"  [Event] Author: {event.author}, Type: {type(event).__name__}, Final: {event.is_final_response()}, Content: {event.content}\")\n",
    "\n",
    "      # Key Concept: is_final_response() marks the concluding message for the turn.\n",
    "      if event.is_final_response():\n",
    "          if event.content and event.content.parts:\n",
    "             # Assuming text response in the first part\n",
    "             final_response_text = event.content.parts[0].text\n",
    "          elif event.actions and event.actions.escalate: # Handle potential errors/escalations\n",
    "             final_response_text = f\"Agent escalated: {event.error_message or 'No specific message.'}\"\n",
    "          # Add more checks here if needed (e.g., specific error codes)\n",
    "          break # Stop processing events once the final response is found\n",
    "\n",
    "  print(f\"<<< Agent Response: {final_response_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21205683",
   "metadata": {},
   "source": [
    "### Running the Conversation\n",
    "\n",
    "Since `call_agent_async` is asynchronous, we don't want the output to mix the text between multiple agent calls. That's why we have the `await` keyword to tell python to wait until the response is fully done to move up to the next call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "66e474c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> User Query: What is the weather like in London?\n",
      "--- Tool: get_weather called for city: London ---\n",
      "<<< Agent Response: The weather in London is cloudy with a temperature of 15째C.\n",
      "\n",
      "\n",
      ">>> User Query: How about Paris?\n",
      "--- Tool: get_weather called for city: Paris ---\n",
      "<<< Agent Response: Sorry, I don't have weather information for Paris.\n",
      "\n",
      "\n",
      ">>> User Query: Tell me the weather in New York\n",
      "--- Tool: get_weather called for city: New York ---\n",
      "<<< Agent Response: The weather in New York is sunny with a temperature of 25째C.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title Run the Initial Conversation\n",
    "\n",
    "async def run_conversation():\n",
    "    await call_agent_async(\"What is the weather like in London?\",\n",
    "                                       runner=runner,\n",
    "                                       user_id=USER_ID,\n",
    "                                       session_id=SESSION_ID)\n",
    "\n",
    "    await call_agent_async(\"How about Paris?\",\n",
    "                                       runner=runner,\n",
    "                                       user_id=USER_ID,\n",
    "                                       session_id=SESSION_ID) # Expecting the tool's error message\n",
    "\n",
    "    await call_agent_async(\"Tell me the weather in New York\",\n",
    "                                       runner=runner,\n",
    "                                       user_id=USER_ID,\n",
    "                                       session_id=SESSION_ID)\n",
    "\n",
    "# Execute the conversation using await in an async context (like Colab/Jupyter)\n",
    "await run_conversation()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2eea3f",
   "metadata": {},
   "source": [
    "# Building an Agent Team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "52687075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greeting and Farewell tools defined\n"
     ]
    }
   ],
   "source": [
    "# These will be the tools that the agents can use\n",
    "\n",
    "def say_hello(name: str = \"World\") -> str:\n",
    "    \"\"\"Provides a simple greeting for a given name\n",
    "\n",
    "    Args:\n",
    "        name (str, optional): The name of the person to greet.\n",
    "\n",
    "    Returns:\n",
    "        str: A friendly greeting message\n",
    "    \"\"\"\n",
    "    print(f\"--- Tool: [say_hello] called with name: {name} ---\")\n",
    "    \n",
    "    return f\"Hello, {name}!\"\n",
    "\n",
    "def say_goodbye(name: str = \"World\") -> str:\n",
    "    \"\"\"Provides a goodbye message for a given name\n",
    "\n",
    "    Args: \n",
    "        name (str, optional): The name of the person to bid farewell\n",
    "\n",
    "    Returns:\n",
    "        str: A farewell message to conclude the conversation\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"--- Tool: [say_goodbye] called with name: {name} ---\")\n",
    "    \n",
    "    return f\"Goodbye, {name}! Have a splendid day!\"\n",
    "\n",
    "print(\"Greeting and Farewell tools defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cad9d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
